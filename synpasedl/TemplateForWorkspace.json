{
	"$schema": "http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
	"contentVersion": "1.0.0.0",
	"parameters": {
		"workspaceName": {
			"type": "string",
			"metadata": "Workspace name",
			"defaultValue": "synpasedl"
		},
		"RestService1_password": {
			"type": "secureString",
			"metadata": "Secure string for 'password' of 'RestService1'"
		},
		"bbaccstorageold_accountKey": {
			"type": "secureString",
			"metadata": "Secure string for 'accountKey' of 'bbaccstorageold'"
		},
		"defaultstorage_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'defaultstorage'"
		},
		"jcinput_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'jcinput'"
		},
		"lipsearch_key": {
			"type": "secureString",
			"metadata": "Secure string for 'key' of 'lipsearch'"
		},
		"lipstorage_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'lipstorage'"
		},
		"synpasedl-WorkspaceDefaultSqlServer_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'synpasedl-WorkspaceDefaultSqlServer'"
		},
		"RestService1_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://www.yahoo.com"
		},
		"RestService1_properties_typeProperties_userName": {
			"type": "string",
			"defaultValue": "anonymous"
		},
		"bbaccstorageold_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://accsynapsestorage.dfs.core.windows.net/"
		},
		"isd_sasUri": {
			"type": "secureString",
			"metadata": "Secure string for 'sasUri' of 'isd'"
		},
		"lipsearch_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://lipsearch.search.windows.net"
		},
		"lipstore_properties_typeProperties_serviceEndpoint": {
			"type": "string",
			"defaultValue": "https://lipstore.blob.core.windows.net/"
		},
		"mlopskey1_properties_typeProperties_baseUrl": {
			"type": "string",
			"defaultValue": "https://mlopskeyv1.vault.azure.net/"
		},
		"nyc_tlc_fhv_sasUri": {
			"type": "secureString",
			"metadata": "Secure string for 'sasUri' of 'nyc_tlc_fhv'"
		},
		"nyc_tlc_green_sasUri": {
			"type": "secureString",
			"metadata": "Secure string for 'sasUri' of 'nyc_tlc_green'"
		},
		"nyc_tlc_yellow_sasUri": {
			"type": "secureString",
			"metadata": "Secure string for 'sasUri' of 'nyc_tlc_yellow'"
		},
		"synpasedl-WorkspaceDefaultStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://synpasedlstore.dfs.core.windows.net"
		}
	},
	"variables": {
		"workspaceId": "[concat('Microsoft.Synapse/workspaces/', parameters('workspaceName'))]"
	},
	"resources": [
		{
			"name": "[concat(parameters('workspaceName'), '/azuresearch')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "CopytoSearch",
						"type": "Copy",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "DelimitedTextSource",
								"storeSettings": {
									"type": "AzureBlobStorageReadSettings",
									"recursive": true,
									"enablePartitionDiscovery": false
								},
								"formatSettings": {
									"type": "DelimitedTextReadSettings"
								}
							},
							"sink": {
								"type": "AzureSearchIndexSink",
								"writeBatchSize": 1000,
								"writeBehavior": "merge"
							},
							"enableStaging": false,
							"translator": {
								"type": "TabularTranslator",
								"typeConversion": true,
								"typeConversionSettings": {
									"allowDataTruncation": true,
									"treatBooleanAsNumber": false
								}
							}
						},
						"inputs": [
							{
								"referenceName": "lipdata",
								"type": "DatasetReference",
								"parameters": {}
							}
						],
						"outputs": [
							{
								"referenceName": "lipsearch",
								"type": "DatasetReference",
								"parameters": {}
							}
						]
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/lipdata')]",
				"[concat(variables('workspaceId'), '/datasets/lipsearch')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/claytonvideosjc')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "ClaytonHomesVideodata",
						"type": "Copy",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "BinarySource",
								"storeSettings": {
									"type": "AzureBlobStorageReadSettings",
									"recursive": true
								},
								"formatSettings": {
									"type": "BinaryReadSettings"
								}
							},
							"sink": {
								"type": "BinarySink",
								"storeSettings": {
									"type": "AzureBlobStorageWriteSettings"
								}
							},
							"enableStaging": false
						},
						"inputs": [
							{
								"referenceName": "jcinput",
								"type": "DatasetReference",
								"parameters": {}
							}
						],
						"outputs": [
							{
								"referenceName": "localstoreclayton",
								"type": "DatasetReference",
								"parameters": {}
							}
						]
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/jcinput')]",
				"[concat(variables('workspaceId'), '/datasets/localstoreclayton')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/tpchdata')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "tpchdata",
						"type": "Copy",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "BinarySource",
								"storeSettings": {
									"type": "AzureBlobFSReadSettings",
									"recursive": true
								},
								"formatSettings": {
									"type": "BinaryReadSettings"
								}
							},
							"sink": {
								"type": "BinarySink",
								"storeSettings": {
									"type": "AzureBlobFSWriteSettings"
								}
							},
							"enableStaging": false
						},
						"inputs": [
							{
								"referenceName": "oldtpch",
								"type": "DatasetReference",
								"parameters": {}
							}
						],
						"outputs": [
							{
								"referenceName": "localstore",
								"type": "DatasetReference",
								"parameters": {}
							}
						]
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/oldtpch')]",
				"[concat(variables('workspaceId'), '/datasets/localstore')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/RestResource1')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "RestService1",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "RestResource",
				"typeProperties": {},
				"schema": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/RestService1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/jcinput')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "jcinput",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "Binary",
				"typeProperties": {
					"location": {
						"type": "AzureBlobStorageLocation",
						"container": "models"
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/jcinput')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/lipdata')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "lipstore",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobStorageLocation",
						"fileName": "audiocall1.txt",
						"container": "lippercalltext"
					},
					"columnDelimiter": "\t",
					"escapeChar": "\\",
					"quoteChar": "\""
				},
				"schema": [
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					},
					{
						"type": "String"
					}
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/lipstore')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/lipsearch')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "lipsearch",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "AzureSearchIndex",
				"schema": [],
				"typeProperties": {
					"indexName": "azureblob-index"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/lipsearch')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/localstore')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "synpasedl-WorkspaceDefaultStorage",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "Binary",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"folderPath": "tpch",
						"fileSystem": "root"
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/synpasedl-WorkspaceDefaultStorage')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/localstoreclayton')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "defaultstorage",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "Binary",
				"typeProperties": {
					"location": {
						"type": "AzureBlobStorageLocation",
						"folderPath": "claytonhomes/models",
						"container": "root"
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/defaultstorage')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/oldtpch')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "bbaccstorageold",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "Binary",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"folderPath": "tpchdata",
						"fileSystem": "synapseroot"
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/bbaccstorageold')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/RestService1')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "RestService",
				"typeProperties": {
					"url": "[parameters('RestService1_properties_typeProperties_url')]",
					"enableServerCertificateValidation": true,
					"authenticationType": "Basic",
					"userName": "[parameters('RestService1_properties_typeProperties_userName')]",
					"password": {
						"type": "SecureString",
						"value": "[parameters('RestService1_password')]"
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/bbaccstorageold')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('bbaccstorageold_properties_typeProperties_url')]",
					"accountKey": {
						"type": "SecureString",
						"value": "[parameters('bbaccstorageold_accountKey')]"
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/defaultstorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobStorage",
				"typeProperties": {
					"connectionString": "[parameters('defaultstorage_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/isd')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobStorage",
				"typeProperties": {
					"sasUri": "[parameters('isd_sasUri')]"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/jcinput')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobStorage",
				"typeProperties": {
					"connectionString": "[parameters('jcinput_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/lipsearch')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureSearch",
				"typeProperties": {
					"url": "[parameters('lipsearch_properties_typeProperties_url')]",
					"key": {
						"type": "SecureString",
						"value": "[parameters('lipsearch_key')]"
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/lipstorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobStorage",
				"typeProperties": {
					"connectionString": "[parameters('lipstorage_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/lipstore')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobStorage",
				"typeProperties": {
					"serviceEndpoint": "[parameters('lipstore_properties_typeProperties_serviceEndpoint')]",
					"accountKind": "StorageV2"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/mlopskey1')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureKeyVault",
				"typeProperties": {
					"baseUrl": "[parameters('mlopskey1_properties_typeProperties_baseUrl')]"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/nyc_tlc_fhv')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobStorage",
				"typeProperties": {
					"sasUri": "[parameters('nyc_tlc_fhv_sasUri')]"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/nyc_tlc_green')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobStorage",
				"typeProperties": {
					"sasUri": "[parameters('nyc_tlc_green_sasUri')]"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/nyc_tlc_yellow')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobStorage",
				"typeProperties": {
					"sasUri": "[parameters('nyc_tlc_yellow_sasUri')]"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/synpasedl-WorkspaceDefaultSqlServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('synpasedl-WorkspaceDefaultSqlServer_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/synpasedl-WorkspaceDefaultStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('synpasedl-WorkspaceDefaultStorage_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AutoResolveIntegrationRuntime')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "Managed",
				"typeProperties": {
					"computeProperties": {
						"location": "AutoResolve",
						"dataFlowProperties": {
							"computeType": "General",
							"coreCount": 8,
							"timeToLive": 0
						}
					}
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/WorkspaceSystemIdentity')]",
			"type": "Microsoft.Synapse/workspaces/credentials",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "ManagedIdentity",
				"typeProperties": {}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/bbfpdopvconf')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "create database datagovdb;\n\nuse datagovdb;\nCREATE LOGIN [bbfpdopv] FROM EXTERNAL PROVIDER;\n\nuse datagovdb;\nCREATE USER [bbfpdopv] FOR LOGIN [bbfpdopv];\nALTER ROLE db_datareader ADD MEMBER [bbfpdopv];\n\nGRANT REFERENCES ON DATABASE SCOPED CREDENTIAL::[scoped_credential] TO [bbfpdopv];",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "datagovdb",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/tpschexttablecreation')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "IF NOT EXISTS (SELECT * FROM sys.external_file_formats WHERE name = 'SynapseParquetFormat') \n\tCREATE EXTERNAL FILE FORMAT [SynapseParquetFormat] \n\tWITH ( FORMAT_TYPE = PARQUET)\nGO\n\nIF NOT EXISTS (SELECT * FROM sys.external_data_sources WHERE name = 'root_synpasedlstore_dfs_core_windows_net') \n\tCREATE EXTERNAL DATA SOURCE [root_synpasedlstore_dfs_core_windows_net] \n\tWITH (\n\t\tLOCATION = 'abfss://root@synpasedlstore.dfs.core.windows.net' \n\t)\nGO\n\nCREATE EXTERNAL TABLE Nation (\n\t[N_NATIONKEY] numeric(38,0),\n\t[N_NAME] nvarchar(4000),\n\t[N_REGIONKEY] numeric(38,0),\n\t[N_COMMENT] nvarchar(4000)\n\t)\n\tWITH (\n\tLOCATION = 'tpch/NATION/**',\n\tDATA_SOURCE = [root_synpasedlstore_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM dbo.Nation\nGO\n\nSELECT\n    TOP 100 *\nFROM\n    OPENROWSET(\n        BULK 'https://synpasedlstore.dfs.core.windows.net/root/tpch/PART/**',\n        FORMAT = 'PARQUET'\n    ) AS [result]\n\n\nIF NOT EXISTS (SELECT * FROM sys.external_file_formats WHERE name = 'SynapseParquetFormat') \n\tCREATE EXTERNAL FILE FORMAT [SynapseParquetFormat] \n\tWITH ( FORMAT_TYPE = PARQUET)\nGO\n\nIF NOT EXISTS (SELECT * FROM sys.external_data_sources WHERE name = 'root_synpasedlstore_dfs_core_windows_net') \n\tCREATE EXTERNAL DATA SOURCE [root_synpasedlstore_dfs_core_windows_net] \n\tWITH (\n\t\tLOCATION = 'abfss://root@synpasedlstore.dfs.core.windows.net' \n\t)\nGO\n\nCREATE EXTERNAL TABLE customer (\n\t[C_CUSTKEY] numeric(9,0),\n\t[C_NAME] nvarchar(4000),\n\t[C_ADDRESS] nvarchar(4000),\n\t[C_NATIONKEY] numeric(38,0),\n\t[C_PHONE] nvarchar(4000),\n\t[C_ACCTBAL] numeric(9,2),\n\t[C_MKTSEGMENT] nvarchar(4000),\n\t[C_COMMENT] nvarchar(4000)\n\t)\n\tWITH (\n\tLOCATION = 'tpch/CUSTOMER/**',\n\tDATA_SOURCE = [root_synpasedlstore_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT\n    TOP 100 *\nFROM\n    OPENROWSET(\n        BULK 'https://synpasedlstore.dfs.core.windows.net/root/tpch/PART/**',\n        FORMAT = 'PARQUET'\n    ) AS [result]\n\nSELECT\n    TOP 100 *\nFROM\n    OPENROWSET(\n        BULK 'https://synpasedlstore.dfs.core.windows.net/root/tpch/ORDERS/**',\n        FORMAT = 'PARQUET'\n    ) AS [result]\n\nCreate View PartView as (\nSELECT  * FROM\nOPENROWSET( BULK N'https://synpasedlstore.dfs.core.windows.net/root/tpch/PART/**', FORMAT = 'PARQUET') \nWITH (\n      P_PARTKEY DECIMAL(12,0), \n      P_NAME varchar(MAX),\n      P_MFGR varchar(max),\n\t  P_BRAND varchar(1000),\n\t  P_TYPE varchar(100),\n\t  P_CONTAINER varchar(50),\n\t  P_RETAILPRICE numeric(9,0),\n\t  P_COMMENT varchar(max)\n) as rows\n)\n\nSelect top 100 * from PartView\n\nCreate View NationView as (\nSELECT  * FROM\nOPENROWSET( BULK N'https://synpasedlstore.dfs.core.windows.net/root/tpch/NATION/**', FORMAT = 'PARQUET') \nWITH (\n    [N_NATIONKEY] numeric(38,0),\n\t[N_NAME] nvarchar(4000),\n\t[N_REGIONKEY] numeric(38,0),\n\t[N_COMMENT] nvarchar(4000)\n) as rows\n)\n\nCreate View CustomerView as (\nSELECT  * FROM\nOPENROWSET( BULK N'https://synpasedlstore.dfs.core.windows.net/root/tpch/CUSTOMER/**', FORMAT = 'PARQUET') \nWITH (\n\t[C_CUSTKEY] numeric(9,0),\n\t[C_NAME] nvarchar(4000),\n\t[C_ADDRESS] nvarchar(4000),\n\t[C_NATIONKEY] numeric(38,0),\n\t[C_PHONE] nvarchar(4000),\n\t[C_ACCTBAL] numeric(9,2),\n\t[C_MKTSEGMENT] nvarchar(4000),\n\t[C_COMMENT] nvarchar(4000)\n) as rows\n)\n\nCreate View OrderView as (\nSELECT  * FROM\nOPENROWSET( BULK N'https://synpasedlstore.dfs.core.windows.net/root/tpch/ORDERS/**', FORMAT = 'PARQUET') \nWITH (\n\t[O_ORDERKEY] decimal(12,0),\n\t[O_CUSTKEY] decimal(12,0),\n\t[O_ORDERSTATUS] nvarchar(100),\n\t[O_TOTALPRICE] numeric(38,0),\n\t[O_ORDERDATE] DATE,\n\t[O_ORDERPRIORITY] varchar(200),\n\t[O_CLERK] nvarchar(100),\n\t[O_SHIPPRIORITY] decimal(38,0),\n\t[O_COMMENT] varchar(max)\n) as rows\n)\n\n\nSELECT\n    TOP 100 *\nFROM\n    OPENROWSET(\n        BULK 'https://synpasedlstore.dfs.core.windows.net/root/tpch/LINEITEM/**',\n        FORMAT = 'PARQUET'\n    ) AS [result]\n\nCreate View LineitemView as (\nSELECT * FROM\nOPENROWSET( BULK N'https://synpasedlstore.dfs.core.windows.net/root/tpch/LINEITEM/**', FORMAT = 'PARQUET') \nWITH (\n\t[L_ORDERKEY] decimal(12,0),\n\t[L_PARTKEY] decimal(12,0),\n\t[L_SUPPKEY] decimal(12,0),\n\t[L_LINENUMBER] decimal(12,0),\n\t[L_QUANTITY] decimal(12,0),\n\t[L_EXTENDEDPRICE] numeric(38,0),\n\t[L_DISCOUNT] numeric(38,0),\n\t[L_TAX] numeric(38,0),\n\t[L_RETURNFLAG] varchar(200),\n\t[L_LINESTATUS] varchar(200),\n\t[L_COMMITDATE] DATE,\n\t[L_RECEIPTDATE] DATE,\n\t[L_SHIPINSTRUCT] varchar(max),\n\t[L_SHIPMODE] varchar(max)\n) as rows\n)\n\n\nSELECT\n    TOP 100 *\nFROM\n    OPENROWSET(\n        BULK 'https://synpasedlstore.dfs.core.windows.net/root/tpch/PARTSUPP/**',\n        FORMAT = 'PARQUET'\n    ) AS [result]\n\ndrop view PartSuppView;\nCreate View PartSuppView as (\nSELECT * FROM\nOPENROWSET( BULK N'https://synpasedlstore.dfs.core.windows.net/root/tpch/PARTSUPP/**', FORMAT = 'PARQUET') \nWITH (\n    [PS_PARTKEY] decimal(38,0),\n\t[PS_SUPPKEY] decimal(38,0),\n\t[PS_AVAILQTY] decimal(38,0),\n\t[PS_SUPPLYCOST] decimal(38,4),\n\t[PS_COMMENT] nvarchar(4000)\n) as rows\n)\n\nCreate View RegionView as (\nSELECT\n    *\nFROM\n    OPENROWSET(\n        BULK 'https://synpasedlstore.dfs.core.windows.net/root/tpch/REGION/**',\n        FORMAT = 'PARQUET'\n    ) AS [result]\n)\n\nCreate View SupplierView as (\nSELECT\n    *\nFROM\n    OPENROWSET(\n        BULK 'https://synpasedlstore.dfs.core.windows.net/root/tpch/SUPPLIER/**',\n        FORMAT = 'PARQUET'\n    ) AS [result]\n)\n\nselect Top 100 * from SupplierView\n\n\n\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "datagovdb",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Data Exploration and ML Modeling - NYC taxi predict using Spark MLlib')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "spark32",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "224g",
					"driverCores": 32,
					"executorMemory": "224g",
					"executorCores": 32,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "b77c1cb1-2d1f-447f-9d72-c1dd243678e2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/80ef7369-572a-4abd-b09a-033367f44858/resourceGroups/mlopsdeveast/providers/Microsoft.Synapse/workspaces/synpasedl/bigDataPools/spark32",
						"name": "spark32",
						"type": "Spark",
						"endpoint": "https://synpasedl.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/spark32",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 10,
						"cores": 32,
						"memory": 224
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"# Predict NYC Taxi Tips using Spark ML and Azure Open Datasets\n",
							"\n",
							"The notebook ingests, visualizes, prepares and then trains a model based on an Open Dataset that tracks NYC Yellow Taxi trips and various attributes around them.\n",
							"The goal is to predict for a given trip whether there will be a tip or not.\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"import matplotlib.pyplot as plt\n",
							"\n",
							"from pyspark.sql.functions import unix_timestamp\n",
							"\n",
							"from pyspark.sql import SparkSession\n",
							"from pyspark.sql.types import *\n",
							"from pyspark.sql.functions import *\n",
							"\n",
							"from pyspark.ml import Pipeline\n",
							"from pyspark.ml import PipelineModel\n",
							"from pyspark.ml.feature import RFormula\n",
							"from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorIndexer\n",
							"from pyspark.ml.classification import LogisticRegression\n",
							"from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
							"from pyspark.ml.evaluation import BinaryClassificationEvaluator"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Ingest Data¶ \n",
							"\n",
							"Get a sample data of nyc yellow taxi to make it faster/easier to evaluate different approaches to prep for the modelling phase later in the notebook."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"# Import NYC yellow cab data from Azure Open Datasets\n",
							"from azureml.opendatasets import NycTlcYellow\n",
							"\n",
							"from datetime import datetime\n",
							"from dateutil import parser\n",
							"\n",
							"end_date = parser.parse('2018-05-08 00:00:00')\n",
							"start_date = parser.parse('2018-05-01 00:00:00')\n",
							"\n",
							"nyc_tlc = NycTlcYellow(start_date=start_date, end_date=end_date)\n",
							"nyc_tlc_df = nyc_tlc.to_spark_dataframe()"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"source": [
							"#To make development easier, faster and less expensive downsample for now\n",
							"sampled_taxi_df = nyc_tlc_df.sample(True, 0.001, seed=1234)"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Exploratory Data Analysis\n",
							"\n",
							"Look at the data and evaluate its suitability for use in a model, do this via some basic charts focussed on tip values and relationships."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"#The charting package needs a Pandas dataframe or numpy array do the conversion\n",
							"sampled_taxi_pd_df = sampled_taxi_df.toPandas()\n",
							"\n",
							"# Look at tips by amount count histogram\n",
							"ax1 = sampled_taxi_pd_df['tipAmount'].plot(kind='hist', bins=25, facecolor='lightblue')\n",
							"ax1.set_title('Tip amount distribution')\n",
							"ax1.set_xlabel('Tip Amount ($)')\n",
							"ax1.set_ylabel('Counts')\n",
							"plt.suptitle('')\n",
							"plt.show()\n",
							"\n",
							"# How many passengers tip'd by various amounts\n",
							"ax2 = sampled_taxi_pd_df.boxplot(column=['tipAmount'], by=['passengerCount'])\n",
							"ax2.set_title('Tip amount by Passenger count')\n",
							"ax2.set_xlabel('Passenger count') \n",
							"ax2.set_ylabel('Tip Amount ($)')\n",
							"plt.suptitle('')\n",
							"plt.show()\n",
							"\n",
							"# Look at the relationship between fare and tip amounts\n",
							"ax = sampled_taxi_pd_df.plot(kind='scatter', x= 'fareAmount', y = 'tipAmount', c='blue', alpha = 0.10, s=2.5*(sampled_taxi_pd_df['passengerCount']))\n",
							"ax.set_title('Tip amount by Fare amount')\n",
							"ax.set_xlabel('Fare Amount ($)')\n",
							"ax.set_ylabel('Tip Amount ($)')\n",
							"plt.axis([-2, 80, -2, 20])\n",
							"plt.suptitle('')\n",
							"plt.show()"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Data Prep and Featurization\n",
							"\n",
							"It's clear from the visualizations above that there are a bunch of outliers in the data. These will need to be filtered out in addition there are extra variables that are not going to be useful in the model we build at the end.\n",
							"\n",
							"Finally there is a need to create some new (derived) variables that will work better with the model.\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"taxi_df = sampled_taxi_df.select('totalAmount', 'fareAmount', 'tipAmount', 'paymentType', 'rateCodeId', 'passengerCount'\\\n",
							"                                , 'tripDistance', 'tpepPickupDateTime', 'tpepDropoffDateTime'\\\n",
							"                                , date_format('tpepPickupDateTime', 'hh').alias('pickupHour')\\\n",
							"                                , date_format('tpepPickupDateTime', 'EEEE').alias('weekdayString')\\\n",
							"                                , (unix_timestamp(col('tpepDropoffDateTime')) - unix_timestamp(col('tpepPickupDateTime'))).alias('tripTimeSecs')\\\n",
							"                                , (when(col('tipAmount') > 0, 1).otherwise(0)).alias('tipped')\n",
							"                                )\\\n",
							"                        .filter((sampled_taxi_df.passengerCount > 0) & (sampled_taxi_df.passengerCount < 8)\\\n",
							"                                & (sampled_taxi_df.tipAmount >= 0) & (sampled_taxi_df.tipAmount <= 25)\\\n",
							"                                & (sampled_taxi_df.fareAmount >= 1) & (sampled_taxi_df.fareAmount <= 250)\\\n",
							"                                & (sampled_taxi_df.tipAmount < sampled_taxi_df.fareAmount)\\\n",
							"                                & (sampled_taxi_df.tripDistance > 0) & (sampled_taxi_df.tripDistance <= 100)\\\n",
							"                                & (sampled_taxi_df.rateCodeId <= 5)\n",
							"                                & (sampled_taxi_df.paymentType.isin({\"1\", \"2\"}))\n",
							"                                )"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Data Prep and Featurization Part 2\n",
							"\n",
							"Having created new variables its now possible to drop the columns they were derived from so that the dataframe that goes into the model is the smallest in terms of number of variables, that is required.\n",
							"\n",
							"Also create some more features based on new columns from the first round.\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"taxi_featurised_df = taxi_df.select('totalAmount', 'fareAmount', 'tipAmount', 'paymentType', 'passengerCount'\\\n",
							"                                                , 'tripDistance', 'weekdayString', 'pickupHour','tripTimeSecs','tipped'\\\n",
							"                                                , when((taxi_df.pickupHour <= 6) | (taxi_df.pickupHour >= 20),\"Night\")\\\n",
							"                                                .when((taxi_df.pickupHour >= 7) & (taxi_df.pickupHour <= 10), \"AMRush\")\\\n",
							"                                                .when((taxi_df.pickupHour >= 11) & (taxi_df.pickupHour <= 15), \"Afternoon\")\\\n",
							"                                                .when((taxi_df.pickupHour >= 16) & (taxi_df.pickupHour <= 19), \"PMRush\")\\\n",
							"                                                .otherwise(0).alias('trafficTimeBins')\n",
							"                                              )\\\n",
							"                                       .filter((taxi_df.tripTimeSecs >= 30) & (taxi_df.tripTimeSecs <= 7200))"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Encoding\n",
							"\n",
							"Different ML algorithms support different types of input, for this example Logistic Regression is being used for Binary Classification. This means that any Categorical (string) variables must be converted to numbers.\n",
							"\n",
							"The process is not as simple as a \"map\" style function as the relationship between the numbers can introduce a bias in the resulting model, the approach is to index the variable and then encode using a std approach called One Hot Encoding.\n",
							"\n",
							"This approach requires the encoder to \"learn\"/fit a model over the data in the Spark instance and then transform based on what was learnt.\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"# The sample uses an algorithm that only works with numeric features convert them so they can be consumed\n",
							"sI1 = StringIndexer(inputCol=\"trafficTimeBins\", outputCol=\"trafficTimeBinsIndex\"); \n",
							"en1 = OneHotEncoder(dropLast=False, inputCol=\"trafficTimeBinsIndex\", outputCol=\"trafficTimeBinsVec\");\n",
							"sI2 = StringIndexer(inputCol=\"weekdayString\", outputCol=\"weekdayIndex\"); \n",
							"en2 = OneHotEncoder(dropLast=False, inputCol=\"weekdayIndex\", outputCol=\"weekdayVec\");\n",
							"\n",
							"# Create a new dataframe that has had the encodings applied\n",
							"encoded_final_df = Pipeline(stages=[sI1, en1, sI2, en2]).fit(taxi_featurised_df).transform(taxi_featurised_df)"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Generation of Testing and Training Data Sets\n",
							"Simple split, 70% for training and 30% for testing the model. Playing with this ratio may result in different models.\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"# Decide on the split between training and testing data from the dataframe \n",
							"trainingFraction = 0.7\n",
							"testingFraction = (1-trainingFraction)\n",
							"seed = 1234\n",
							"\n",
							"# Split the dataframe into test and training dataframes\n",
							"train_data_df, test_data_df = encoded_final_df.randomSplit([trainingFraction, testingFraction], seed=seed)"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Train the Model\n",
							"\n",
							"Train the Logistic Regression model and then evaluate it using Area under ROC as the metric."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"## Create a new LR object for the model\n",
							"logReg = LogisticRegression(maxIter=10, regParam=0.3, labelCol = 'tipped')\n",
							"\n",
							"## The formula for the model\n",
							"classFormula = RFormula(formula=\"tipped ~ pickupHour + weekdayVec + passengerCount + tripTimeSecs + tripDistance + fareAmount + paymentType+ trafficTimeBinsVec\")\n",
							"\n",
							"## Undertake training and create an LR model\n",
							"lrModel = Pipeline(stages=[classFormula, logReg]).fit(train_data_df)\n",
							"\n",
							"## Saving the model is optional but its another for of inter session cache\n",
							"datestamp = datetime.now().strftime('%m-%d-%Y-%s');\n",
							"fileName = \"lrModel_\" + datestamp;\n",
							"logRegDirfilename = fileName;\n",
							"lrModel.save(logRegDirfilename)\n",
							"\n",
							"## Predict tip 1/0 (yes/no) on the test dataset, evaluation using AUROC\n",
							"predictions = lrModel.transform(test_data_df)\n",
							"predictionAndLabels = predictions.select(\"prediction\", \"label\").rdd\n",
							"metrics = BinaryClassificationMetrics(predictionAndLabels)\n",
							"print(\"Area under ROC = %s\" % metrics.areaUnderROC)"
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Evaluate and Visualize\n",
							"\n",
							"Plot the actual curve to develop a better understanding of the model.\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"## Plot the ROC curve, no need for pandas as this uses the modelSummary object\n",
							"modelSummary = lrModel.stages[-1].summary\n",
							"\n",
							"plt.plot([0, 1], [0, 1], 'r--')\n",
							"plt.plot(modelSummary.roc.select('FPR').collect(),\n",
							"         modelSummary.roc.select('TPR').collect())\n",
							"plt.xlabel('False Positive Rate')\n",
							"plt.ylabel('True Positive Rate')\n",
							"plt.show()"
						],
						"outputs": [],
						"execution_count": 10
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Using Azure Open Datasets in Synapse')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "75454d26-2887-4e18-945c-f019ef203ed3"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"# Using Azure Open Datasets in Synapse - Enrich NYC Green Taxi Data with Holiday and Weather\n",
							"\n",
							"Synapse has [Azure Open Datasets](https://azure.microsoft.com/en-us/services/open-datasets/) package pre-installed. This notebook provides examples of how to enrich NYC Green Taxi Data with Holiday and Weather with focusing on :\n",
							"- read Azure Open Dataset\n",
							"- manipulate the data to prepare for further analysis, including column projection, filtering, grouping and joins etc. \n",
							"- create a Spark table to be used in other notebooks for modeling training"
						]
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Data loading \n",
							"Let's first load the [NYC green taxi trip records](https://azure.microsoft.com/en-us/services/open-datasets/catalog/nyc-taxi-limousine-commission-green-taxi-trip-records/). The Open Datasets package contains a class representing each data source (NycTlcGreen for example) to easily filter date parameters before downloading."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"from azureml.opendatasets import NycTlcGreen\n",
							"\n",
							"from datetime import datetime\n",
							"from dateutil import parser\n",
							"end_date = parser.parse('2018-06-06')\n",
							"start_date = parser.parse('2018-05-01')\n",
							"\n",
							"nyc_tlc = NycTlcGreen(start_date=start_date, end_date=end_date)\n",
							"nyc_tlc_df = nyc_tlc.to_spark_dataframe()"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"source": [
							"# Display 5 rows\n",
							"\n",
							"nyc_tlc_df.show(5, truncate = False)"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "markdown",
						"source": [
							"Now that the initial data is loaded. Let's do some projection on the data to \n",
							"- create new columns for the month number, day of month, day of week, and hour of day. These info is going to be used in the training model to factor in time-based seasonality.\n",
							"- add a static feature for the country code to join holiday data. "
						]
					},
					{
						"cell_type": "code",
						"source": [
							"# Extract month, day of month, and day of week from pickup datetime and add a static column for the country code to join holiday data. \n",
							"\n",
							"import pyspark.sql.functions as f\n",
							"\n",
							"nyc_tlc_df_expand = nyc_tlc_df.withColumn('datetime',f.to_date('lpepPickupDatetime'))\\\n",
							"                .withColumn('month_num',f.month(nyc_tlc_df.lpepPickupDatetime))\\\n",
							"                .withColumn('day_of_month',f.dayofmonth(nyc_tlc_df.lpepPickupDatetime))\\\n",
							"                .withColumn('day_of_week',f.dayofweek(nyc_tlc_df.lpepPickupDatetime))\\\n",
							"                .withColumn('hour_of_day',f.hour(nyc_tlc_df.lpepPickupDatetime))\\\n",
							"                .withColumn('country_code',f.lit('US'))"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "markdown",
						"source": [
							"Remove some of the columns that won't need for modeling or additional feature building.\n",
							"\n",
							"\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"# Remove unused columns from nyc green taxi data\n",
							"\n",
							"columns_to_remove = [\"lpepDropoffDatetime\", \"puLocationId\", \"doLocationId\", \"pickupLongitude\", \n",
							"                     \"pickupLatitude\", \"dropoffLongitude\",\"dropoffLatitude\" ,\"rateCodeID\", \n",
							"                     \"storeAndFwdFlag\",\"paymentType\", \"fareAmount\", \"extra\", \"mtaTax\",\n",
							"                     \"improvementSurcharge\", \"tollsAmount\", \"ehailFee\", \"tripType \"  \n",
							"                    ]\n",
							"\n",
							"nyc_tlc_df_clean = nyc_tlc_df_expand.select([column for column in nyc_tlc_df_expand.columns if column not in columns_to_remove])"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"source": [
							"# Display 5 rows\n",
							"nyc_tlc_df_clean.show(5)"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Enrich with holiday data\n",
							"Now that we have taxi data downloaded and roughly prepared, add in holiday data as additional features. Holiday-specific features will assist model accuracy, as major holidays are times where taxi demand increases dramatically and supply becomes limited. \n",
							"\n",
							"Let's load the [public holidays](https://azure.microsoft.com/en-us/services/open-datasets/catalog/public-holidays/) from Azure Open datasets.\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"from azureml.opendatasets import PublicHolidays\n",
							"\n",
							"hol = PublicHolidays(start_date=start_date, end_date=end_date)\n",
							"hol_df = hol.to_spark_dataframe()\n",
							"\n",
							"# Display data\n",
							"hol_df.show(5, truncate = False)"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "markdown",
						"source": [
							"Rename the countryRegionCode and date columns to match the respective field names from the taxi data, and also normalize the time so it can be used as a key. "
						]
					},
					{
						"cell_type": "code",
						"source": [
							"hol_df_clean = hol_df.withColumnRenamed('countryRegionCode','country_code')\\\n",
							"            .withColumn('datetime',f.to_date('date'))\n",
							"\n",
							"hol_df_clean.show(5)"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "markdown",
						"source": [
							"Next, join the holiday data with the taxi data by performing a left-join. This will preserve all records from taxi data, but add in holiday data where it exists for the corresponding datetime and country_code, which in this case is always \"US\". Preview the data to verify that they were merged correctly."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"# enrich taxi data with holiday data\n",
							"nyc_taxi_holiday_df = nyc_tlc_df_clean.join(hol_df_clean, on = ['datetime', 'country_code'] , how = 'left')\n",
							"\n",
							"nyc_taxi_holiday_df.show(5)"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"source": [
							"# Create a temp table and filter out non empty holiday rows\n",
							"\n",
							"nyc_taxi_holiday_df.createOrReplaceTempView(\"nyc_taxi_holiday_df\")\n",
							"spark.sql(\"SELECT * from nyc_taxi_holiday_df WHERE holidayName is NOT NULL \").show(5, truncate = False)"
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Enrich with weather data¶\n",
							"\n",
							"Now we append NOAA surface weather data to the taxi and holiday data. Use a similar approach to fetch the [NOAA weather history data](https://azure.microsoft.com/en-us/services/open-datasets/catalog/noaa-integrated-surface-data/) from Azure Open Datasets. "
						]
					},
					{
						"cell_type": "code",
						"source": [
							"from azureml.opendatasets import NoaaIsdWeather\n",
							"\n",
							"isd = NoaaIsdWeather(start_date, end_date)\n",
							"isd_df = isd.to_spark_dataframe()"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"source": [
							"isd_df.show(5, truncate = False)"
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"source": [
							"# Filter out weather info for new york city, remove the recording with null temperature \n",
							"\n",
							"weather_df = isd_df.filter(isd_df.latitude >= '40.53')\\\n",
							"                        .filter(isd_df.latitude <= '40.88')\\\n",
							"                        .filter(isd_df.longitude >= '-74.09')\\\n",
							"                        .filter(isd_df.longitude <= '-73.72')\\\n",
							"                        .filter(isd_df.temperature.isNotNull())\\\n",
							"                        .withColumnRenamed('datetime','datetime_full')\n",
							"                         "
						],
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "code",
						"source": [
							"# Remove unused columns\n",
							"\n",
							"columns_to_remove_weather = [\"usaf\", \"wban\", \"longitude\", \"latitude\"]\n",
							"weather_df_clean = weather_df.select([column for column in weather_df.columns if column not in columns_to_remove_weather])\\\n",
							"                        .withColumn('datetime',f.to_date('datetime_full'))\n",
							"\n",
							"weather_df_clean.show(5, truncate = False)"
						],
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "markdown",
						"source": [
							"Next group the weather data so that you have daily aggregated weather values. \n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"# Enrich weather data with aggregation statistics\n",
							"\n",
							"aggregations = {\"snowDepth\": \"mean\", \"precipTime\": \"max\", \"temperature\": \"mean\", \"precipDepth\": \"max\"}\n",
							"weather_df_grouped = weather_df_clean.groupby(\"datetime\").agg(aggregations)"
						],
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "code",
						"source": [
							"weather_df_grouped.show(5)"
						],
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "code",
						"source": [
							"# Rename columns\n",
							"\n",
							"weather_df_grouped = weather_df_grouped.withColumnRenamed('avg(snowDepth)','avg_snowDepth')\\\n",
							"                                       .withColumnRenamed('avg(temperature)','avg_temperature')\\\n",
							"                                       .withColumnRenamed('max(precipTime)','max_precipTime')\\\n",
							"                                       .withColumnRenamed('max(precipDepth)','max_precipDepth')"
						],
						"outputs": [],
						"execution_count": 16
					},
					{
						"cell_type": "markdown",
						"source": [
							"Merge the taxi and holiday data you prepared with the new weather data. This time you only need the datetime key, and again perform a left-join of the data. Run the describe() function on the new dataframe to see summary statistics for each field."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"# enrich taxi data with weather\n",
							"nyc_taxi_holiday_weather_df = nyc_taxi_holiday_df.join(weather_df_grouped, on = 'datetime' , how = 'left')\n",
							"nyc_taxi_holiday_weather_df.cache()"
						],
						"outputs": [],
						"execution_count": 17
					},
					{
						"cell_type": "code",
						"source": [
							"nyc_taxi_holiday_weather_df.show(5)"
						],
						"outputs": [],
						"execution_count": 18
					},
					{
						"cell_type": "code",
						"metadata": {
							"collapsed": false
						},
						"source": [
							"# Run the describe() function on the new dataframe to see summary statistics for each field.\n",
							"\n",
							"display(nyc_taxi_holiday_weather_df.describe())"
						],
						"outputs": [],
						"execution_count": 19
					},
					{
						"cell_type": "markdown",
						"source": [
							"The summary statistics shows that the totalAmount field has negative values, which don't make sense in the context.\n",
							"\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"# Remove invalid rows with less than 0 taxi fare or tip\n",
							"final_df = nyc_taxi_holiday_weather_df.filter(nyc_taxi_holiday_weather_df.tipAmount > 0)\\\n",
							"                                      .filter(nyc_taxi_holiday_weather_df.totalAmount > 0)"
						],
						"outputs": [],
						"execution_count": 20
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Cleaning up the existing Database\n",
							"\n",
							"First we need to drop the tables since Spark requires that a database is empty before we can drop the Database.\n",
							"\n",
							"Then we recreate the database and set the default database context to it."
						]
					},
					{
						"cell_type": "code",
						"source": [
							"spark.sql(\"DROP TABLE IF EXISTS NYCTaxi.nyc_taxi_holiday_weather\"); "
						],
						"outputs": [],
						"execution_count": 21
					},
					{
						"cell_type": "code",
						"source": [
							"spark.sql(\"DROP DATABASE IF EXISTS NYCTaxi\"); \n",
							"spark.sql(\"CREATE DATABASE NYCTaxi\"); \n",
							"spark.sql(\"USE NYCTaxi\");"
						],
						"outputs": [],
						"execution_count": 22
					},
					{
						"cell_type": "markdown",
						"source": [
							"## Creating a new table\n",
							"We create a nyc_taxi_holiday_weather table from the nyc_taxi_holiday_weather dataframe.\n",
							""
						]
					},
					{
						"cell_type": "code",
						"source": [
							"from pyspark.sql import SparkSession\n",
							"from pyspark.sql.types import *\n",
							"\n",
							"final_df.write.saveAsTable(\"nyc_taxi_holiday_weather\");\n",
							"spark.sql(\"SELECT COUNT(*) FROM nyc_taxi_holiday_weather\").show();"
						],
						"outputs": [],
						"execution_count": 23
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/downloadtpch')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "spark32",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "224g",
					"driverCores": 32,
					"executorMemory": "224g",
					"executorCores": 32,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "50a2217f-f7ac-4eb2-9ca6-d488647717bf"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/80ef7369-572a-4abd-b09a-033367f44858/resourceGroups/mlopsdeveast/providers/Microsoft.Synapse/workspaces/synpasedl/bigDataPools/spark32",
						"name": "spark32",
						"type": "Spark",
						"endpoint": "https://synpasedl.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/spark32",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 10,
						"cores": 32,
						"memory": 224
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"pip install wget"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import wget\r\n",
							"url = 'https://drive.google.com/file/d/14fyJnKFkCPcQw1q9dG0BE1t7RNNmYP8C/view?usp=sharing'\r\n",
							"#filename = wget.download(url)\r\n",
							"\r\n",
							"output_directory = 'tpch'\r\n",
							"filename = wget.download(url, out=output_directory)"
						],
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from notebookutils import mssparkutils\r\n",
							"mssparkutils.fs.ls('/')"
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/fhvnoaadatasave')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "spark32",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "0e7ad72d-c1cb-4ee6-9b05-0e8d113496e1"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/80ef7369-572a-4abd-b09a-033367f44858/resourceGroups/mlopsdeveast/providers/Microsoft.Synapse/workspaces/synpasedl/bigDataPools/spark32",
						"name": "spark32",
						"type": "Spark",
						"endpoint": "https://synpasedl.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/spark32",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 10,
						"cores": 32,
						"memory": 224
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "python"
							},
							"collapsed": false
						},
						"source": [
							"%%pyspark\n",
							"blob_account_name = \"azureopendatastorage\"\n",
							"blob_container_name = \"nyctlc\"\n",
							"blob_relative_path = \"fhv\"\n",
							"blob_sas_token = r\"\"\n",
							"# Allow SPARK to read from Blob remotely\n",
							"wasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\n",
							"\n",
							"spark.conf.set(\n",
							"    'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n",
							"    blob_sas_token)\n",
							"df = spark.read.parquet(wasbs_path)\n",
							"display(df.limit(10))"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df.count()"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df.repartition(5).write.mode(\"overwrite\").parquet('abfss://root@synpasedlstore.dfs.core.windows.net/nycfhv/')"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from azureml.opendatasets import NoaaIsdWeather\r\n",
							"\r\n",
							"data = NoaaIsdWeather()\r\n",
							"#df1 = data.to_spark_dataframe()\r\n",
							"# Display 10 rows\r\n",
							"#display(df1.limit(10))\r\n",
							"# no data "
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df1.count()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df.repartition(1).write.mode(\"overwrite\").parquet('abfss://root@synpasedlstore.dfs.core.windows.net/noaa/')"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/nyxtaxidatasave')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "spark32",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "224g",
					"driverCores": 32,
					"executorMemory": "224g",
					"executorCores": 32,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "74a8231a-0d36-4dff-8fb3-f4b882fb4a87"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/80ef7369-572a-4abd-b09a-033367f44858/resourceGroups/mlopsdeveast/providers/Microsoft.Synapse/workspaces/synpasedl/bigDataPools/spark32",
						"name": "spark32",
						"type": "Spark",
						"endpoint": "https://synpasedl.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/spark32",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 10,
						"cores": 32,
						"memory": 224
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import matplotlib.pyplot as plt\r\n",
							"\r\n",
							"from pyspark.sql.functions import unix_timestamp\r\n",
							"\r\n",
							"from pyspark.sql import SparkSession\r\n",
							"from pyspark.sql.types import *\r\n",
							"from pyspark.sql.functions import *\r\n",
							"\r\n",
							"from pyspark.ml import Pipeline\r\n",
							"from pyspark.ml import PipelineModel\r\n",
							"from pyspark.ml.feature import RFormula\r\n",
							"from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorIndexer\r\n",
							"from pyspark.ml.classification import LogisticRegression\r\n",
							"from pyspark.mllib.evaluation import BinaryClassificationMetrics\r\n",
							"from pyspark.ml.evaluation import BinaryClassificationEvaluator"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Import NYC yellow cab data from Azure Open Datasets\r\n",
							"from azureml.opendatasets import NycTlcYellow\r\n",
							"\r\n",
							"from datetime import datetime\r\n",
							"from dateutil import parser\r\n",
							"\r\n",
							"end_date = parser.parse('2018-12-31 00:00:00')\r\n",
							"start_date = parser.parse('2018-01-01 00:00:00')\r\n",
							"\r\n",
							"nyc_tlc = NycTlcYellow(start_date=start_date, end_date=end_date)\r\n",
							"nyc_tlc_df = nyc_tlc.to_spark_dataframe()"
						],
						"outputs": [],
						"execution_count": 19
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"nyc_tlc_df.count()"
						],
						"outputs": [],
						"execution_count": 20
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"nyc_tlc_df.repartition(5).write.mode(\"overwrite\").parquet('abfss://root@synpasedlstore.dfs.core.windows.net/yellowtaxi/')"
						],
						"outputs": [],
						"execution_count": 23
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from azureml.opendatasets import NycTlcGreen\r\n",
							"\r\n",
							"from datetime import datetime\r\n",
							"from dateutil import parser\r\n",
							"end_date = parser.parse('2018-12-31')\r\n",
							"start_date = parser.parse('2018-01-01')\r\n",
							"\r\n",
							"nyc_tlc = NycTlcGreen(start_date=start_date, end_date=end_date)\r\n",
							"nyc_tlc_dfg = nyc_tlc.to_spark_dataframe()"
						],
						"outputs": [],
						"execution_count": 26
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"nyc_tlc_dfg.count()"
						],
						"outputs": [],
						"execution_count": 27
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"nyc_tlc_dfg.repartition(1).write.mode(\"overwrite\").parquet('abfss://root@synpasedlstore.dfs.core.windows.net/greentaxi/')"
						],
						"outputs": [],
						"execution_count": 29
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/searchml')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "spark32",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "224g",
					"driverCores": 32,
					"executorMemory": "224g",
					"executorCores": 32,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "cd42a89f-e98c-45e7-bbd8-415032a09b49"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/80ef7369-572a-4abd-b09a-033367f44858/resourceGroups/mlopsdeveast/providers/Microsoft.Synapse/workspaces/synpasedl/bigDataPools/spark32",
						"name": "spark32",
						"type": "Spark",
						"endpoint": "https://synpasedl.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/spark32",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 10,
						"cores": 32,
						"memory": 224
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"%%configure -f\r\n",
							"{\r\n",
							"  \"name\": \"synapseml\",\r\n",
							"  \"conf\": {\r\n",
							"      \"spark.jars.packages\": \"com.microsoft.azure:synapseml_2.12:0.10.0\",\r\n",
							"      \"spark.jars.repositories\": \"https://mmlspark.azureedge.net/maven\",\r\n",
							"      \"spark.jars.excludes\": \"org.scala-lang:scala-reflect,org.apache.spark:spark-tags_2.12,org.scalactic:scalactic_2.12,org.scalatest:scalatest_2.12,com.fasterxml.jackson.core:jackson-databind\",\r\n",
							"      \"spark.yarn.user.classpath.first\": \"true\"\r\n",
							"  }\r\n",
							"}"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#cogsvc = token_library.GetSecret(\"mlopskeyv1\", \"cogsvc\", \"mlopskey1\")\r\n",
							"#cogsvc = TokenLibrary.getSecretWithLS(\"mlopskeyv1\", \"cogsvc\")\r\n",
							"# getSecret(akvName: String, secret: String, linkedService: String)\r\n",
							"# cogsvc = TokenLibrary.getSecret(\"mlopskeyv1\", \"cogsvc\", \"mlopskey1\")\r\n",
							"# https://learn.microsoft.com/en-us/azure/search/search-synapseml-cognitive-services\r\n",
							"cogsvc= mssparkutils.credentials.getSecret(\"mlopskeyv1\", \"cogsvc\")\r\n",
							"cogsearchname= mssparkutils.credentials.getSecret(\"mlopskeyv1\", \"cogsearchname\")\r\n",
							"cogsearchkey= mssparkutils.credentials.getSecret(\"mlopskeyv1\", \"cogsearchkey\")\r\n",
							"#print(cogsvc)"
						],
						"outputs": [],
						"execution_count": 21
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import os\r\n",
							"from pyspark.sql.functions import udf, trim, split, explode, col, monotonically_increasing_id, lit\r\n",
							"from pyspark.sql.types import StringType\r\n",
							"from synapse.ml.core.spark import FluentAPI\r\n",
							"\r\n",
							"cognitive_services_key = cogsvc\r\n",
							"cognitive_services_region = \"eastus2\"\r\n",
							"\r\n",
							"search_service = cogsearchname\r\n",
							"search_key = cogsearchkey\r\n",
							"search_index = \"synapseindex\""
						],
						"outputs": [],
						"execution_count": 22
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"def blob_to_url(blob):\r\n",
							"    [prefix, postfix] = blob.split(\"@\")\r\n",
							"    container = prefix.split(\"/\")[-1]\r\n",
							"    split_postfix = postfix.split(\"/\")\r\n",
							"    account = split_postfix[0]\r\n",
							"    filepath = \"/\".join(split_postfix[1:])\r\n",
							"    return \"https://{}/{}/{}\".format(account, container, filepath)\r\n",
							"\r\n",
							"\r\n",
							"df2 = (spark.read.format(\"binaryFile\")\r\n",
							"    .load(\"wasbs://ignite2021@mmlsparkdemo.blob.core.windows.net/form_subset/*\")\r\n",
							"    .select(\"path\")\r\n",
							"    .limit(10)\r\n",
							"    .select(udf(blob_to_url, StringType())(\"path\").alias(\"url\"))\r\n",
							"    .cache())\r\n",
							"    \r\n",
							"display(df2)"
						],
						"outputs": [],
						"execution_count": 23
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"from synapse.ml.cognitive import AnalyzeInvoices\r\n",
							"\r\n",
							"analyzed_df = (AnalyzeInvoices()\r\n",
							"    .setSubscriptionKey(cognitive_services_key)\r\n",
							"    .setLocation(cognitive_services_region)\r\n",
							"    .setImageUrlCol(\"url\")\r\n",
							"    .setOutputCol(\"invoices\")\r\n",
							"    .setErrorCol(\"errors\")\r\n",
							"    .setConcurrency(5)\r\n",
							"    .transform(df2)\r\n",
							"    .cache())\r\n",
							"\r\n",
							"display(analyzed_df)"
						],
						"outputs": [],
						"execution_count": 24
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"from synapse.ml.cognitive import FormOntologyLearner\r\n",
							"\r\n",
							"itemized_df = (FormOntologyLearner()\r\n",
							"    .setInputCol(\"invoices\")\r\n",
							"    .setOutputCol(\"extracted\")\r\n",
							"    .fit(analyzed_df)\r\n",
							"    .transform(analyzed_df)\r\n",
							"    .select(\"url\", \"extracted.*\").select(\"*\", explode(col(\"Items\")).alias(\"Item\"))\r\n",
							"    .drop(\"Items\").select(\"Item.*\", \"*\").drop(\"Item\"))\r\n",
							"\r\n",
							"display(itemized_df)"
						],
						"outputs": [],
						"execution_count": 25
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"from synapse.ml.cognitive import Translate\r\n",
							"\r\n",
							"translated_df = (Translate()\r\n",
							"    .setSubscriptionKey(cognitive_services_key)\r\n",
							"    .setLocation(cognitive_services_region)\r\n",
							"    .setTextCol(\"Description\")\r\n",
							"    .setErrorCol(\"TranslationError\")\r\n",
							"    .setOutputCol(\"output\")\r\n",
							"    .setToLanguage([\"zh-Hans\", \"fr\", \"ru\", \"cy\"])\r\n",
							"    .setConcurrency(5)\r\n",
							"    .transform(itemized_df)\r\n",
							"    .withColumn(\"Translations\", col(\"output.translations\")[0])\r\n",
							"    .drop(\"output\", \"TranslationError\")\r\n",
							"    .cache())\r\n",
							"\r\n",
							"display(translated_df)"
						],
						"outputs": [],
						"execution_count": 27
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from synapse.ml.cognitive import *\r\n",
							"\r\n",
							"(translated_df.withColumn(\"DocID\", monotonically_increasing_id().cast(\"string\"))\r\n",
							"    .withColumn(\"SearchAction\", lit(\"upload\"))\r\n",
							"    .writeToAzureSearch(\r\n",
							"        subscriptionKey=search_key,\r\n",
							"        actionCol=\"SearchAction\",\r\n",
							"        serviceName=search_service,\r\n",
							"        indexName=search_index,\r\n",
							"        keyCol=\"DocID\",\r\n",
							"    ))"
						],
						"outputs": [],
						"execution_count": 28
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import requests\r\n",
							"\r\n",
							"url = \"https://{}.search.windows.net/indexes/{}/docs/search?api-version=2020-06-30\".format(search_service, search_index)\r\n",
							"requests.post(url, json={\"search\": \"door\", \"count\": \"true\", \"select\": \"Description, Translations\"}, headers={\"api-key\": search_key}).json()"
						],
						"outputs": [],
						"execution_count": 29
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/spark32')]",
			"type": "Microsoft.Synapse/workspaces/bigDataPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"autoPause": {
					"enabled": true,
					"delayInMinutes": 15
				},
				"autoScale": {
					"enabled": true,
					"maxNodeCount": 20,
					"minNodeCount": 3
				},
				"nodeCount": 10,
				"nodeSize": "XLarge",
				"nodeSizeFamily": "MemoryOptimized",
				"sparkVersion": "3.2",
				"isComputeIsolationEnabled": false,
				"sessionLevelPackagesEnabled": true,
				"annotations": []
			},
			"dependsOn": [],
			"location": "eastus"
		}
	]
}