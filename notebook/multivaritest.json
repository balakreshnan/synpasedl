{
	"name": "multivaritest",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "spark32",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "56g",
			"driverCores": 8,
			"executorMemory": "56g",
			"executorCores": 8,
			"numExecutors": 5,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "5",
				"spark.dynamicAllocation.maxExecutors": "5",
				"spark.autotune.trackingId": "ea1ed851-26dc-4325-b43c-ef53d9f716fb"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/80ef7369-572a-4abd-b09a-033367f44858/resourceGroups/mlopsdeveast/providers/Microsoft.Synapse/workspaces/synpasedl/bigDataPools/spark32",
				"name": "spark32",
				"type": "Spark",
				"endpoint": "https://synpasedl.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/spark32",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.2",
				"nodeCount": 10,
				"cores": 32,
				"memory": 224,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%%configure -f\r\n",
					"{\r\n",
					"  \"name\": \"synapseml\",\r\n",
					"  \"conf\": {\r\n",
					"      \"spark.jars.packages\": \" com.microsoft.azure:synapseml_2.12:0.9.5 \",\r\n",
					"      \"spark.jars.repositories\": \"https://mmlspark.azureedge.net/maven\",\r\n",
					"      \"spark.jars.excludes\": \"org.scala-lang:scala-reflect,org.apache.spark:spark-tags_2.12,org.scalactic:scalactic_2.12,org.scalatest:scalatest_2.12,io.netty:netty-tcnative-boringssl-static\",\r\n",
					"      \"spark.yarn.user.classpath.first\": \"true\"\r\n",
					"  }\r\n",
					"}"
				],
				"execution_count": 1
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%%configure -f\r\n",
					"{\r\n",
					"  \"name\": \"synapseml\",\r\n",
					"  \"conf\": {\r\n",
					"      \"spark.jars.packages\": \"com.microsoft.azure:synapseml_2.12:0.10.2\",\r\n",
					"      \"spark.jars.repositories\": \"https://mmlspark.azureedge.net/maven\",\r\n",
					"      \"spark.jars.excludes\": \"org.scala-lang:scala-reflect,org.apache.spark:spark-tags_2.12,org.scalactic:scalactic_2.12,org.scalatest:scalatest_2.12,com.fasterxml.jackson.core:jackson-databind\",\r\n",
					"      \"spark.yarn.user.classpath.first\": \"true\"\r\n",
					"  }\r\n",
					"}"
				],
				"execution_count": 2
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"from synapse.ml.cognitive import *\r\n",
					"from notebookutils import mssparkutils\r\n",
					"import numpy as np\r\n",
					"import pandas as pd\r\n",
					"import pyspark\r\n",
					"from pyspark.sql.functions import col\r\n",
					"from pyspark.sql.functions import lit\r\n",
					"from pyspark.sql.types import DoubleType\r\n",
					"import synapse.ml"
				],
				"execution_count": 19
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"microsoft": {
						"language": "python"
					},
					"collapsed": false
				},
				"source": [
					"%%pyspark\r\n",
					"df = spark.read.load('abfss://metricfolder@synpasedlstore.dfs.core.windows.net/sensor.csv', format='csv'\r\n",
					"## If header exists uncomment line below\r\n",
					", header=True\r\n",
					")\r\n",
					"display(df.limit(10))"
				],
				"execution_count": 20
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#!wget https://sparkdemostorage.blob.core.windows.net/mvadcsvdata/spark-demo-data.csv"
				],
				"execution_count": 11
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#df = spark.read.format(\"csv\").option(\"header\", True).load(\"wasbs://mvadcsvdata@sparkdemostorage.blob.core.windows.net/spark-demo-data.csv\")"
				],
				"execution_count": 10
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#df = spark.read.format(\"csv\").option(\"header\", True).load(\"wasbs://metricfolder@synpasedlstore.blob.core.windows.net/train_set.csv\")\r\n",
					"df = df.withColumn(\"sensor_01\", col(\"sensor_01\").cast(DoubleType())) \\\r\n",
					"    .withColumn(\"sensor_02\", col(\"sensor_02\").cast(DoubleType())) \\\r\n",
					"    .withColumn(\"sensor_03\", col(\"sensor_03\").cast(DoubleType()))\r\n",
					"\r\n",
					"df.show(10)"
				],
				"execution_count": 13
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"import os\r\n",
					"from pyspark.sql import SparkSession\r\n",
					"from synapse.ml.core.platform import find_secret\r\n",
					"\r\n",
					"# Bootstrap Spark Session\r\n",
					"spark = SparkSession.builder.getOrCreate()"
				],
				"execution_count": 23
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# An Anomaly Dectector subscription key\r\n",
					"#anomalyKey = find_secret(\"anomaly-api-key\")\r\n",
					"anomalyKey = mssparkutils.credentials.getSecret(\"mlopskeyv1\",\"anomalykey\")\r\n",
					"# Your storage account name\r\n",
					"storageName = \"synpasedlstore\"\r\n",
					"# A connection string to your blob storage account\r\n",
					"#storageKey = find_secret(\"madtest-storage-key\")\r\n",
					"storageKey = \"l86dmmORzfrNQN1LcNosdYFaE1ET+ohUBWfaNXabC3nny5OfJWBDsK0T3HC+gMbSdH5kexDnJhFZ+AStrv675g==\"\r\n",
					"# A place to save intermediate MVAD results\r\n",
					"intermediateSaveDir = (\r\n",
					"    \"wasbs://metricfolder@synpasedlstore.blob.core.windows.net/intermediateData\"\r\n",
					")\r\n",
					"# The location of the anomaly detector resource that you created\r\n",
					"location = \"eastus\""
				],
				"execution_count": 25
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"spark.sparkContext._jsc.hadoopConfiguration().set(\r\n",
					"    f\"fs.azure.account.key.{storageName}.blob.core.windows.net\", storageKey\r\n",
					")"
				],
				"execution_count": 26
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"import numpy as np\r\n",
					"import pandas as pd\r\n",
					"\r\n",
					"import pyspark\r\n",
					"from pyspark.sql.functions import col\r\n",
					"from pyspark.sql.functions import lit\r\n",
					"from pyspark.sql.types import DoubleType\r\n",
					"import matplotlib.pyplot as plt\r\n",
					"\r\n",
					"import synapse.ml\r\n",
					"from synapse.ml.cognitive import *"
				],
				"execution_count": 27
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"df = (\r\n",
					"    spark.read.format(\"csv\")\r\n",
					"    .option(\"header\", \"true\")\r\n",
					"    .load(\"wasbs://publicwasb@mmlspark.blob.core.windows.net/MVAD/sample.csv\")\r\n",
					")"
				],
				"execution_count": 28
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"df = (\r\n",
					"    df.withColumn(\"sensor_1\", col(\"sensor_1\").cast(DoubleType()))\r\n",
					"    .withColumn(\"sensor_2\", col(\"sensor_2\").cast(DoubleType()))\r\n",
					"    .withColumn(\"sensor_3\", col(\"sensor_3\").cast(DoubleType()))\r\n",
					")\r\n",
					"\r\n",
					"# Let's inspect the dataframe:\r\n",
					"df.show(5)"
				],
				"execution_count": 29
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"trainingStartTime = \"2020-06-01T12:00:00Z\"\r\n",
					"trainingEndTime = \"2020-07-02T17:55:00Z\"\r\n",
					"timestampColumn = \"timestamp\"\r\n",
					"inputColumns = [\"sensor_1\", \"sensor_2\", \"sensor_3\"]\r\n",
					"\r\n",
					"estimator = (\r\n",
					"    FitMultivariateAnomaly()\r\n",
					"    .setSubscriptionKey(anomalyKey)\r\n",
					"    .setLocation(location)\r\n",
					"    .setStartTime(trainingStartTime)\r\n",
					"    .setEndTime(trainingEndTime)\r\n",
					"    .setIntermediateSaveDir(intermediateSaveDir)\r\n",
					"    .setTimestampCol(timestampColumn)\r\n",
					"    .setInputCols(inputColumns)\r\n",
					"    .setSlidingWindow(200)\r\n",
					")"
				],
				"execution_count": 30
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"model = estimator.fit(df)"
				],
				"execution_count": 31
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"inferenceStartTime = \"2020-07-02T18:00:00Z\"\r\n",
					"inferenceEndTime = \"2020-07-06T05:15:00Z\"\r\n",
					"\r\n",
					"result = (\r\n",
					"    model.setStartTime(inferenceStartTime)\r\n",
					"    .setEndTime(inferenceEndTime)\r\n",
					"    .setOutputCol(\"results\")\r\n",
					"    .setErrorCol(\"errors\")\r\n",
					"    .setInputCols(inputColumns)\r\n",
					"    .setTimestampCol(timestampColumn)\r\n",
					"    .transform(df)\r\n",
					")\r\n",
					"\r\n",
					"result.show(5)"
				],
				"execution_count": 32
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"rdf = (\r\n",
					"    result.select(\r\n",
					"        \"timestamp\",\r\n",
					"        *inputColumns,\r\n",
					"        \"results.contributors\",\r\n",
					"        \"results.isAnomaly\",\r\n",
					"        \"results.severity\"\r\n",
					"    )\r\n",
					"    .orderBy(\"timestamp\", ascending=True)\r\n",
					"    .filter(col(\"timestamp\") >= lit(inferenceStartTime))\r\n",
					"    .toPandas()\r\n",
					")\r\n",
					"\r\n",
					"rdf"
				],
				"execution_count": 33
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def parse(x):\r\n",
					"    if type(x) is list:\r\n",
					"        return dict([item[::-1] for item in x])\r\n",
					"    else:\r\n",
					"        return {\"series_0\": 0, \"series_1\": 0, \"series_2\": 0}\r\n",
					"\r\n",
					"\r\n",
					"rdf[\"contributors\"] = rdf[\"contributors\"].apply(parse)\r\n",
					"rdf = pd.concat(\r\n",
					"    [rdf.drop([\"contributors\"], axis=1), pd.json_normalize(rdf[\"contributors\"])], axis=1\r\n",
					")\r\n",
					"rdf"
				],
				"execution_count": 34
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"minSeverity = 0.1\r\n",
					"\r\n",
					"\r\n",
					"####### Main Figure #######\r\n",
					"plt.figure(figsize=(23, 8))\r\n",
					"plt.plot(\r\n",
					"    rdf[\"timestamp\"],\r\n",
					"    rdf[\"sensor_1\"],\r\n",
					"    color=\"tab:orange\",\r\n",
					"    line,\r\n",
					"    linewidth=2,\r\n",
					"    label=\"sensor_1\",\r\n",
					")\r\n",
					"plt.plot(\r\n",
					"    rdf[\"timestamp\"],\r\n",
					"    rdf[\"sensor_2\"],\r\n",
					"    color=\"tab:green\",\r\n",
					"    line,\r\n",
					"    linewidth=2,\r\n",
					"    label=\"sensor_2\",\r\n",
					")\r\n",
					"plt.plot(\r\n",
					"    rdf[\"timestamp\"],\r\n",
					"    rdf[\"sensor_3\"],\r\n",
					"    color=\"tab:blue\",\r\n",
					"    line,\r\n",
					"    linewidth=2,\r\n",
					"    label=\"sensor_3\",\r\n",
					")\r\n",
					"plt.grid(axis=\"y\")\r\n",
					"plt.tick_params(axis=\"x\", which=\"both\", bottom=False, labelbottom=False)\r\n",
					"plt.legend()\r\n",
					"\r\n",
					"anoms = list(rdf[\"severity\"] >= minSeverity)\r\n",
					"_, _, ymin, ymax = plt.axis()\r\n",
					"plt.vlines(np.where(anoms), ymin=ymin, ymax=ymax, color=\"r\", alpha=0.8)\r\n",
					"\r\n",
					"plt.legend()\r\n",
					"plt.title(\r\n",
					"    \"A plot of the values from the three sensors with the detected anomalies highlighted in red.\"\r\n",
					")\r\n",
					"plt.show()\r\n",
					"\r\n",
					"####### Severity Figure #######\r\n",
					"plt.figure(figsize=(23, 1))\r\n",
					"plt.tick_params(axis=\"x\", which=\"both\", bottom=False, labelbottom=False)\r\n",
					"plt.plot(\r\n",
					"    rdf[\"timestamp\"],\r\n",
					"    rdf[\"severity\"],\r\n",
					"    color=\"black\",\r\n",
					"    line,\r\n",
					"    linewidth=2,\r\n",
					"    label=\"Severity score\",\r\n",
					")\r\n",
					"plt.plot(\r\n",
					"    rdf[\"timestamp\"],\r\n",
					"    [minSeverity] * len(rdf[\"severity\"]),\r\n",
					"    color=\"red\",\r\n",
					"    line,\r\n",
					"    linewidth=1,\r\n",
					"    label=\"minSeverity\",\r\n",
					")\r\n",
					"plt.grid(axis=\"y\")\r\n",
					"plt.legend()\r\n",
					"plt.ylim([0, 1])\r\n",
					"plt.title(\"Severity of the detected anomalies\")\r\n",
					"plt.show()\r\n",
					"\r\n",
					"####### Contributors Figure #######\r\n",
					"plt.figure(figsize=(23, 1))\r\n",
					"plt.tick_params(axis=\"x\", which=\"both\", bottom=False, labelbottom=False)\r\n",
					"plt.bar(\r\n",
					"    rdf[\"timestamp\"], rdf[\"series_0\"], width=2, color=\"tab:orange\", label=\"sensor_1\"\r\n",
					")\r\n",
					"plt.bar(\r\n",
					"    rdf[\"timestamp\"],\r\n",
					"    rdf[\"series_1\"],\r\n",
					"    width=2,\r\n",
					"    color=\"tab:green\",\r\n",
					"    label=\"sensor_2\",\r\n",
					"    bottom=rdf[\"series_0\"],\r\n",
					")\r\n",
					"plt.bar(\r\n",
					"    rdf[\"timestamp\"],\r\n",
					"    rdf[\"series_2\"],\r\n",
					"    width=2,\r\n",
					"    color=\"tab:blue\",\r\n",
					"    label=\"sensor_3\",\r\n",
					"    bottom=rdf[\"series_0\"] + rdf[\"series_1\"],\r\n",
					")\r\n",
					"plt.grid(axis=\"y\")\r\n",
					"plt.legend()\r\n",
					"plt.ylim([0, 1])\r\n",
					"plt.title(\"The contribution of each sensor to the detected anomaly\")\r\n",
					"plt.show()"
				],
				"execution_count": 36
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"simpleMultiAnomalyEstimator.cleanUpIntermediateData()\r\n",
					"model.cleanUpIntermediateData()"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					""
				],
				"execution_count": null
			}
		]
	}
}