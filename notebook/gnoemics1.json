{
	"name": "gnoemics1",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "spark32",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "224g",
			"driverCores": 32,
			"executorMemory": "224g",
			"executorCores": 32,
			"numExecutors": 3,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "3",
				"spark.dynamicAllocation.maxExecutors": "3",
				"spark.autotune.trackingId": "f74b81ab-5a24-46b6-b3de-c14c04014749"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/80ef7369-572a-4abd-b09a-033367f44858/resourceGroups/mlopsdeveast/providers/Microsoft.Synapse/workspaces/synpasedl/bigDataPools/spark32",
				"name": "spark32",
				"type": "Spark",
				"endpoint": "https://synpasedl.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/spark32",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.2",
				"nodeCount": 10,
				"cores": 32,
				"memory": 224,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"pip install glow"
				],
				"execution_count": 1
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"from pyspark.sql.functions import explode, col, lit, xxhash64\r\n",
					"from math import ceil\r\n",
					"\r\n",
					"# Import glow.py and register Glow package\r\n",
					"import glow\r\n",
					"glow.register(spark)"
				],
				"execution_count": 1
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#https://github.com/microsoft/genomicsnotebook/blob/main/vcf2parquet-conversion/1000genomes/vcf2parquet-1000genomes.ipynb"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Provide your storage account, container and SAS token\r\n",
					"outputStorageAccount = \"synpasedlstore\"\r\n",
					"outputContainer = \"gnenome\"\r\n",
					"outputSAS = \"sp=r&st=2023-01-10T21:08:19Z&se=2023-02-01T05:08:19Z&spr=https&sv=2021-06-08&sr=c&sig=abZitK3pzUimFU2NxMEK5U82PeYSV6NyX6r84CZkLN0%3D\"\r\n",
					"outputDir = \"data\"\r\n",
					"outputSASURL = \"https://synpasedlstore.blob.core.windows.net/gnenome?sp=r&st=2023-01-10T21:08:19Z&se=2023-02-01T05:08:19Z&spr=https&sv=2021-06-08&sr=c&sig=abZitK3pzUimFU2NxMEK5U82PeYSV6NyX6r84CZkLN0%3D\""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Configure session credentials\r\n",
					"# Set up a SAS for a container with public data - no changes needed here (public SAS)\r\n",
					"spark.conf.set(\r\n",
					"  \"fs.azure.sas.dataset.dataset1000genomes.blob.core.windows.net\",\r\n",
					"  \"sv=2019-10-10&si=prod&sr=c&sig=9nzcxaQn0NprMPlSh4RhFQHcXedLQIcFgbERiooHEqM%3D\")\r\n",
					"\r\n",
					"# Set up a SAS for a container to store .parquet files\r\n",
					"spark.conf.set(\r\n",
					"  \"fs.azure.sas.\"+outputContainer+\".\"+outputStorageAccount+\".blob.core.windows.net\", outputSAS)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# DropDuplicates() partitions into 200 pieces (default value)\r\n",
					"# To change default number of partitions change config -  sqlContext.setConf(\"spark.sql.shuffle.partitions\", )\r\n",
					"partitionMax = 1500\r\n",
					"sqlContext.setConf(\"spark.sql.shuffle.partitions\", partitionMax)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Flatten struct columns\r\n",
					"def flattenStructFields(df):\r\n",
					"  flat_cols = [c[0] for c in df.dtypes if c[1][:6] != 'struct']\r\n",
					"  nested_cols = [c[0] for c in df.dtypes if c[1][:6] =='struct']\r\n",
					"  flat_df = df.select(flat_cols + \r\n",
					"                     [col(nc+'.'+c).alias(nc+'_'+c)\r\n",
					"                     for nc in nested_cols\r\n",
					"                     for c in df.select(nc+'.*').columns])\r\n",
					"  return flat_df\r\n",
					"\r\n",
					"# Add empty columns to match schema\r\n",
					"def completeSchema(df, diffSet):\r\n",
					"  full_df = df\r\n",
					"  for column in diffSet:\r\n",
					"    full_df = full_df.withColumn(column.name, lit(None).cast(column.dataType.simpleString()))\r\n",
					"  return full_df\r\n",
					"\r\n",
					"# Transform dataframe with original vcf schema\r\n",
					"def transformVcf(df, toFlatten, toHash, fullSchemaFields):\r\n",
					"  # Drop duplicates\r\n",
					"  dataDedup = df.dropDuplicates()\r\n",
					"     \r\n",
					"  # Add hashId column to identify variants\r\n",
					"  if toHash:\r\n",
					"    hashCols = list(set(data.columns) - {'genotypes'})\r\n",
					"    dataHashed = dataDedup.withColumn('hashId', xxhash64(*hashCols))\r\n",
					"  else:\r\n",
					"    dataHashed = dataDedup\r\n",
					"  \r\n",
					"  # Flatten data - explode on genotypes, create separate column for each genotypes field, add empty columns to match schema to full dataset\r\n",
					"  if not toFlatten:\r\n",
					"    dataFinal = dataHashed\r\n",
					"  else:\r\n",
					"  # Explode and flatten data\r\n",
					"    dataExploded = dataHashed.withColumn('genotypes', explode('genotypes'))\r\n",
					"    dataExplodedFlatten = flattenStructFields(dataExploded)\r\n",
					"  # Find schema for contig dataset and add columns to match full schema\r\n",
					"    contigSet = set(dataExplodedFlatten.schema.fields)\r\n",
					"    diffSet =(fullSchemaFields - contigSet)\r\n",
					"    dataFinal = completeSchema(dataExplodedFlatten, diffSet)\r\n",
					"   \r\n",
					"  return dataFinal"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Create widgets for toFlatten and contigs\r\n",
					"flatOptions = [False, True]\r\n",
					"dbutils.widgets.dropdown(\"flatten\", \"False\", [str(x) for x in flatOptions])\r\n",
					"\r\n",
					"contigOptions =  list(map(str, range(1, 23)))\r\n",
					"contigLiterals = ['X','Y','MT', 'All']\r\n",
					"contigOptions.extend(contigLiterals)\r\n",
					"dbutils.widgets.multiselect(\"contigsToProcess\", \"22\", contigOptions)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Define parameters\r\n",
					"toFlatten = eval(getArgument(\"flatten\"))\r\n",
					"toHash = True\r\n",
					"repartitionCoef = 45 / 1000000 # gives ~20MB .parquet files\r\n",
					"\r\n",
					"# Define contig list\r\n",
					"contigs = getArgument(\"contigsToProcess\").split(\",\")\r\n",
					"if \"All\" in contigs:\r\n",
					"  contigs = contigOptions\r\n",
					"  contigs.remove('All')\r\n",
					"\r\n",
					"# Find schema for full dataset\r\n",
					"sourceAll = \"wasbs://dataset@dataset1000genomes.blob.core.windows.net/release/20130502/ALL.chr*.vcf.gz\"\r\n",
					"dataAll = spark.read\\\r\n",
					"  .format(\"vcf\")\\\r\n",
					"  .option(\"includeSampleIds\", True)\\\r\n",
					"  .option(\"flattenInfoFields\", True)\\\r\n",
					"  .load(sourceAll)\r\n",
					"\r\n",
					"dataAllExploded = dataAll.withColumn('genotypes', explode('genotypes'))\r\n",
					"dataAllExplodedFlatten = flattenStructFields(dataAllExploded)\r\n",
					"fullSet = set(dataAllExplodedFlatten.schema.fields)\r\n",
					"                 \r\n",
					"for contig in contigs:\r\n",
					"  source = \"wasbs://dataset@dataset1000genomes.blob.core.windows.net/release/20130502/ALL.chr\"+contig+\".*.vcf.gz\"\r\n",
					"\r\n",
					"# Load data\r\n",
					"  data = spark.read\\\r\n",
					"    .format(\"vcf\")\\\r\n",
					"    .option(\"includeSampleIds\", True)\\\r\n",
					"    .option(\"flattenInfoFields\", True)\\\r\n",
					"    .load(source)\r\n",
					"  \r\n",
					"  # Define number of partitions, will be used for coalesce later\r\n",
					"  rowCount = data.count()\r\n",
					"  partCount = ceil (repartitionCoef * rowCount)  \r\n",
					"  if partCount > partitionMax:\r\n",
					"    partCount = partitionMax\r\n",
					"\r\n",
					"  dataFinal = transformVcf(data, toFlatten, toHash, fullSet)\r\n",
					"  if not toFlatten:\r\n",
					"    sink = \"wasbs://\"+outputContainer + \"@\" + outputStorageAccount + \".blob.core.windows.net\"+ outputDir + \"/original/chr\"+contig\r\n",
					"  else:\r\n",
					"    sink = \"wasbs://\"+outputContainer + \"@\" + outputStorageAccount + \".blob.core.windows.net\"+ outputDir + \"/flattened/chr\"+contig\r\n",
					"                 \r\n",
					"  dataFinal.coalesce(partCount). \\\r\n",
					"    write. \\\r\n",
					"    mode(\"overwrite\"). \\\r\n",
					"    format(\"parquet\"). \\\r\n",
					"    save(sink)"
				],
				"execution_count": null
			}
		]
	}
}